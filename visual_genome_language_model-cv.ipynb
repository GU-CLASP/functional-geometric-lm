{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on VisualGenome dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all required codes here\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.spatial import distance\n",
    "#import simple_gensim\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Embedding, TimeDistributed\n",
    "from keras.layers import Lambda, Reshape, Activation\n",
    "import keras.backend as K\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The spatial relations dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_preps = ['in', 'on', 'at', 'to', 'above', 'below', 'over', 'under']\n",
    "\n",
    "# Landau English prepositions\n",
    "en_preps = [\n",
    "    # simple spatial relations\n",
    "    'at', 'on', 'in', 'on', 'off',\n",
    "    'out', 'by', 'from', 'to',\n",
    "    'up', 'down', 'over', 'under',\n",
    "    'with', ('within', 'with in'), ('without', 'with out'), 'near',\n",
    "    'neadby', ('into', 'in to'), ('onto', 'on to'), 'toward',\n",
    "    'through', 'throughout', 'underneath', 'along',\n",
    "    'across', ('among', 'amongst'), 'against', 'around',\n",
    "    'about', 'above', ('amid', 'amidst'), 'before',\n",
    "    'behind', 'below', 'beneath', 'between',\n",
    "    'beside', 'outside', 'inside', ('alongside', 'along side'),\n",
    "    'via', 'after', 'upon', \n",
    "    # compounds\n",
    "    ('top', 'on top of'), ('between', 'in between'), ('right', 'to the right of'), ('parallel', 'parallel to'),\n",
    "    ('back', 'in back of'), ('left', 'to the left of'), ('side', 'to the side'), ('perpendicular', 'perpendicular to'),\n",
    "    ('front', 'in front of'),\n",
    "    # temporal only\n",
    "    'during', 'since', 'until', 'ago',\n",
    "    # intransitivies (+ additional variations)\n",
    "    'here', 'outward', ('backward', 'backwards'), ('south' , 'south of'),\n",
    "    'there', ('afterward', 'afterwards'), 'away', ('east', 'east of'),\n",
    "    'upward', 'upstairs', 'apart', ('west', 'west of'),\n",
    "    'downward', 'downstairs', 'together', 'left',\n",
    "    'inward', 'sideways', ('north', 'north of'), 'right',\n",
    "]\n",
    "\n",
    "\n",
    "# Herskovits projective_terms\n",
    "en_preps += [(w2, w1+' the '+w2+' of')           for w1 in ['at', 'on', 'to', 'by'] for w2 in ['left', 'right'] ]\n",
    "en_preps += [(w2, w1+' the '+w2+' side of')      for w1 in ['at', 'on', 'in', 'to', 'by'] for w2 in ['left', 'right']]\n",
    "en_preps += [(w2, w1+' the '+w2+' hand side of') for w1 in ['at', 'on', 'in', 'to', 'by'] for w2 in ['left', 'right']]\n",
    "en_preps += [(w2, w1+' the '+w2+' of')           for w1 in ['at', 'on', 'in', 'to', 'by'] for w2 in ['front', 'back', 'side']]\n",
    "en_preps += [(w1, 'in '+w1+' of')                for w1 in ['front', 'back']]\n",
    "en_preps += [(w1,)                               for w1 in ['before', 'behind']]\n",
    "en_preps += [(w1, w1+' of')                      for w1 in ['left', 'right', 'back']]\n",
    "en_preps += [(w1,)                               for w1 in ['above', 'below']]\n",
    "en_preps += [(w1,)                               for w1 in ['over', 'under']]\n",
    "en_preps += [(w2, w1+' the '+w2+' of')           for w1 in ['at', 'on', 'in', 'by'] for w2 in ['top', 'bottom']]\n",
    "en_preps += [(w2, w1+' '+w2+' of')               for w1 in ['on'] for w2 in ['top']]\n",
    "\n",
    "# missing items?\n",
    "en_preps += [('next', 'next to'), ('front', 'on the front of', 'on front of', 'front of')]\n",
    "\n",
    "# for those who lost 'the'\n",
    "en_preps += [(w2, w1+' '+w2+' of')           for w1 in ['at', 'on', 'to', 'by'] for w2 in ['left', 'right'] ]\n",
    "en_preps += [(w2, w1+' '+w2+' side of')      for w1 in ['at', 'on', 'in', 'to', 'by'] for w2 in ['left', 'right']]\n",
    "en_preps += [(w2, w1+' '+w2+' hand side of') for w1 in ['at', 'on', 'in', 'to', 'by'] for w2 in ['left', 'right']]\n",
    "en_preps += [(w2, w1+' '+w2+' of')           for w1 in ['at', 'on', 'in', 'to', 'by'] for w2 in ['front', 'back', 'side']]\n",
    "en_preps += [(w2, w1+' '+w2+' of')           for w1 in ['at', 'on', 'in', 'by'] for w2 in ['top', 'bottom']]\n",
    "\n",
    "# fix the tuple types\n",
    "en_preps = [(w,) if type(w) != tuple else w for w in en_preps]\n",
    "\n",
    "# This will create a ditionary of preposition variations to a simple tocken\n",
    "composit2simple = dict()\n",
    "composit2simple.update({w_alt: w[0] for w in en_preps for w_alt in w})\n",
    "# every key is itself!\n",
    "composit2simple.update({w: w        for w in composit2simple.values()})\n",
    "# fix a common annotation with 'a', 'is', 'are'\n",
    "composit2simple.update({new_w_alt: w     for w_alt, w in composit2simple.items() for new_w_alt in [w_alt + ' a', 'are ' + w_alt + ' a', 'is ' + w_alt + ' a', 'are ' + w_alt, 'is ' + w_alt]}) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests on Visual Genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attributes.json      qa_to_region_mapping.json\t    relationship_alias.txt\r\n",
      "attributes.json.zip  qa_to_region_mapping.json.zip  relationships.json\r\n",
      "image_data.json      question_answers.json\t    relationships.json.zip\r\n",
      "image_data.json.zip  question_answers.json.zip\t    scene_graphs.json\r\n",
      "images2.zip\t     readme_v1_4.txt\t\t    scene_graphs.json.zip\r\n",
      "images.zip\t     region_descriptions.json\t    synsets.json\r\n",
      "object_alias.txt     region_descriptions.json.zip   synsets.json.zip\r\n",
      "objects.json\t     region_graphs.json\t\t    VG_100K\r\n",
      "objects.json.zip     region_graphs.json.zip\t    VG_100K_2\r\n"
     ]
    }
   ],
   "source": [
    "# place the visual genom folder in the same path as this file (or symbolic link to the folder) \n",
    "! ls visual_genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read from file\n",
    "rels_from_file = json.load(open('visual_genome/relationships.json'))\n",
    "\n",
    "# name/names correction for reading content of nodes in the dataset\n",
    "name_extract = lambda x: x['names'][0].lower() if 'names' in x and len(x['names']) else x['name'].lower() if 'name' in x else '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert it into a set of (image, subject, predicate, object)\n",
    "triplets = {\n",
    "    (rels_in_image['image_id'],\n",
    "     name_extract(rel['subject']),\n",
    "     composit2simple[rel['predicate'].lower()] if rel['predicate'].lower() in composit2simple else rel['predicate'].lower(),\n",
    "     name_extract(rel['object']))\n",
    "    for rels_in_image in rels_from_file\n",
    "    for rel in rels_in_image['relationships']\n",
    "    if name_extract(rel['subject']) not in composit2simple and name_extract(rel['object']) not in composit2simple\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2371847, 'people', 'showing', 'tattoos')\n",
      "(2405208, 'lady', 'has', 'ponytail')\n",
      "(2355481, 'hole', 'in', 'road')\n",
      "(2374479, 'elephant', 'in', 'grass')\n",
      "(2330968, 'pants', 'on', 'person')\n",
      "(2368956, 'blackberry', 'on', 'cake')\n",
      "(2328369, 'cop', 'wearing', 'coat')\n",
      "(2403336, 'flower', 'on', 'hat')\n",
      "(2342959, 'man', 'wearing', 'sarong')\n",
      "(2388001, 'wall', 'next', 'tracks')\n",
      "(2319551, 'man', 'on', 'runway')\n",
      "(2405530, 'chin', 'of', 'woman')\n",
      "(2403261, 'giants', 'written on', 'jersey')\n",
      "(2334882, 'door', 'on', 'bus')\n",
      "(2365110, 'female', 'with', 'shirt')\n",
      "(2394538, 'writing', 'on', 'paper')\n",
      "(2404013, 'grass', 'around', 'tree')\n",
      "(2397462, 'white', 'computer', 'keyboard.')\n",
      "(2394730, 'man', 'has', 'balding spot')\n",
      "(2320162, 'nose', 'on', 'face')\n",
      "(2404639, 'cup', 'on', 'table')\n",
      "(2416639, 'brick', 'in', 'brick wall')\n",
      "(2318762, 'man', 'top', 'snow')\n",
      "(2389686, 'nose', 'of', 'cow')\n",
      "(2353872, 'hill', 'in', 'distance')\n",
      "(2382901, 'clock', 'has', 'numbers')\n",
      "(2394018, 'post', 'standing across', 'parking lot')\n",
      "(2317034, 'woman', 'wearing', 'coat')\n",
      "(2383035, 'logo and', 'letters on', 'surfboard')\n",
      "(2363611, 'person', 'along', 'walkway')\n",
      "(2401394, 'sign', 'for', 'bikes only')\n",
      "(2412439, 'batter', 'has', 'helmet')\n",
      "(2407335, 'post', 'of', 'fence')\n",
      "(2390069, 'field', 'of', 'zebras')\n",
      "(2376697, 'motorcycle', 'on', 'sidewalk')\n",
      "(2408827, 'hot dog', 'between', 'bread')\n",
      "(2368412, 'fire extinguisher', 'on', 'wall')\n",
      "(2342449, 'ear', 'of', 'cat')\n",
      "(2377940, 'ground', 'covered', 'snow')\n",
      "(2375143, 'sheep', 'has', 'feet')\n",
      "(2372092, 'windshield', 'on', 'bus')\n",
      "(2407609, 'bottles', 'on', 'table')\n",
      "(2376649, 'bear', 'sees', 'fields')\n",
      "(2317728, 'building', 'along', 'street')\n",
      "(2384001, 'tie', 'around', 'towel')\n",
      "(2320460, 'post card', 'on', 'board')\n",
      "(2317296, 'hill', 'covered in', 'trees')\n",
      "(2410176, 'mane', 'on', 'giraffe')\n",
      "(2353631, 'zebra', 'on', 'dirt')\n",
      "(2366362, 'sky', 'above', 'tower')\n",
      "(2343306, 'number', 'on', 'jersey')\n",
      "(2392931, 'platters', 'on', 'dinner table')\n",
      "(2408574, 'lettuce', 'on', 'sandwich')\n",
      "(2361665, 'water', 'in', 'street')\n",
      "(2410609, 'person', 'standing in', 'grass')\n",
      "(2396120, 'edge', 'of a', 'rim')\n",
      "(2358398, 'person', 'on', 'sidewalk')\n",
      "(2317305, 'button', 'built into', 'cell phone')\n",
      "(2353251, 'he', 'cutting a', 'cake')\n",
      "(1593131, 'bus', 'driving down', 'road')\n",
      "(2392374, 'tail', 'attached to', 'zebra')\n",
      "(2360754, 'leg', 'of', 'cat')\n",
      "(2414142, 'woman', 'on', 'sidewalk')\n",
      "(2328321, 'container', 'over', 'table')\n",
      "(2404625, 'drawers', 'have', 'handles')\n",
      "(2346614, 'woman', 'has a', 'walking stick')\n",
      "(2391098, 'bus', 'has', 'head')\n",
      "(2363033, 'bottle man', 'holding', 'food')\n",
      "(2363284, 'plane', 'top', 'ground')\n",
      "(2409641, 'house', 'on edge of', 'pier')\n",
      "(2335207, 'antenna', 'on', 'boat')\n",
      "(2358000, 'mouth', 'of', 'man')\n",
      "(2315459, 'keys', 'on', 'laptop')\n",
      "(2318865, 'man', 'at', 'beach')\n",
      "(2353487, 'she', 'wearing a', 'coat')\n",
      "(3547, 'footpath', 'over', 'grass')\n",
      "(2319076, 'baked pizza', 'on', 'pizza')\n",
      "(2384910, 'man in blue', 'with hands on', 'knees')\n",
      "(2408242, 'curtain', 'in', 'bedroom')\n",
      "(2338625, 'impala', 'drinking', 'water')\n",
      "(2390755, 'motorcycle', 'in', 'window')\n",
      "(2371579, 'car', 'on', 'street')\n",
      "(2380829, 'woman', 'wearing', 'hat')\n",
      "(2353345, 'dirt', 'in', 'flower bed')\n",
      "(2378072, 'face', 'of', 'cat')\n",
      "(2384669, 'man', 'carrying', 'surfboard')\n",
      "(2368108, 'lawn', 'outside', 'house')\n",
      "(2379631, 'person', 'sitting on', 'snow')\n",
      "(2398499, 'dry stem', 'next', 'flower')\n",
      "(2372612, 'building', 'with', 'clock')\n",
      "(2353203, 'umpire', 'has a', 'mask')\n",
      "(2388628, 'woman', 'using', 'hand')\n",
      "(2348032, 'plant', 'front', 'building')\n",
      "(2338462, 'tire', 'on', 'moped')\n",
      "(2385909, 'grafitti', 'on', 'wall')\n",
      "(2379292, 'arm', 'of', 'girl')\n",
      "(2396394, 'airplane', 'has a', 'blue tail')\n",
      "(2353031, 'window', 'in', 'bedroom')\n",
      "(2387174, 'girl', 'watching a', 'parade')\n",
      "(2406666, 'sign', 'on', 'post')\n",
      "(2342576, 'lady', 'has', 'head')\n",
      "(2378536, '14', 'on', 'rider')\n"
     ]
    }
   ],
   "source": [
    "for i, w in enumerate(triplets):\n",
    "    print(w)\n",
    "    if (i>100):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relations_filter = set(composit2simple.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "triplets_filtered = [\n",
    "    (imgid, trg, rel, lnd)\n",
    "    for imgid, trg, rel, lnd in triplets\n",
    "    if rel in relations_filter\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq_c_rel = Counter([\n",
    "    ((trg, lnd), rel)\n",
    "    for imgid, trg, rel, lnd in triplets_filtered\n",
    "])\n",
    "freq_rel = Counter([\n",
    "    rel\n",
    "    for imgid, trg, rel, lnd in triplets_filtered\n",
    "])\n",
    "logprob_c_given_rel = {\n",
    "    ((trg, lnd), rel): np.log2(freq_c_rel[((trg, lnd), rel)]) - np.log2(freq_rel[rel])\n",
    "    for ((trg, lnd), rel) in freq_c_rel\n",
    "}\n",
    "entropy_rel = {\n",
    "    rel0: sum([-logprob_c_given_rel[((trg, lnd), rel)]*(2**(logprob_c_given_rel[((trg, lnd), rel)])) for ((trg, lnd), rel) in logprob_c_given_rel if rel == rel0])\n",
    "    for rel0 in relations_filter\n",
    "}\n",
    "entropy_rel = {\n",
    "    rel: entropy_rel[rel]\n",
    "    for rel in entropy_rel\n",
    "    if entropy_rel[rel] > 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQQJdCCEaggS6EEI0BAl0IYRoCP8HLmAnYRpNW2UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdd6ca984a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colordict = defaultdict(lambda: '#cccccc', [\n",
    "    (w,'#DD7800') for w in ['at','in','on','over','under']] + [\n",
    "    (w,'#004b89') for w in ['below','above','left','right', 'across']\n",
    "])\n",
    "plt.gcf().subplots_adjust(bottom=0.25)\n",
    "labels, results = list(zip(*sorted(entropy_rel.items(), key=lambda x: x[1])[-36:]))\n",
    "plt.bar(range(len(labels)), results, color=[colordict[w] for w in labels])\n",
    "plt.xticks(range(len(labels)), labels, rotation='vertical')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Language Model Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model definition\n",
    "def build_model(\n",
    "        max_len,\n",
    "        vocab_size,\n",
    "        embedding_size = 50,\n",
    "        memory_size = 50,\n",
    "    ):\n",
    "    \n",
    "    lm = Sequential([\n",
    "        Embedding(vocab_size, embedding_size, input_shape=[max_len+1,]),\n",
    "        LSTM(memory_size, return_sequences=True, ), \n",
    "        TimeDistributed(Dense(vocab_size, activation='softmax')),\n",
    "    ])\n",
    "\n",
    "    lm.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "    return lm\n",
    "\n",
    "# perplexity calculator (for a model lm, and a given sample X)\n",
    "def perplexity(lm, X):\n",
    "    X_test = X[:, :-1]\n",
    "    Y_sparse_test = np.expand_dims(X[:, 1:], 2) # the correct predicitons\n",
    "    # predictions:\n",
    "    Y_hat_test = lm.predict(X_test) # the probabilities for all possiblities\n",
    "    Y_sparse_hat_test = np.array([ # the probabilities of the correct prediction\n",
    "        [\n",
    "            Y_hat_test[sent, word, Y_sparse_test[sent,word,0]]\n",
    "            for word in range(Y_sparse_test.shape[1])\n",
    "        ]\n",
    "        for sent in range(Y_sparse_test.shape[0])\n",
    "    ])\n",
    "\n",
    "    return 2**(np.sum(-np.log2(Y_sparse_hat_test), 1)/np.sum(X_test!=0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_ferq = Counter([\n",
    "    w\n",
    "    for words in triplets\n",
    "    for w in words[1:]\n",
    "])\n",
    "\n",
    "# clean the data by removing the least frequent words from the dataset\n",
    "vocab_ferq = defaultdict(int,(\n",
    "    (w, freq)\n",
    "    for w, freq in vocab_ferq.items()\n",
    "    if freq > 100\n",
    "))\n",
    "\n",
    "vocab_ferq['<pad>'] = 0\n",
    "vocab_ferq['</s>'] = 0\n",
    "vocab_ferq['<s>'] = 0\n",
    "vocab_ferq['<unk>'] = 0\n",
    "\n",
    "vocab = list(vocab_ferq.keys())\n",
    "word2index = defaultdict(lambda: vocab.index('<unk>'), ((w, i) for i, w in enumerate(vocab)))\n",
    "max_len = max(len(s[1:]) for s in triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only accept triplets with a known word in dictionary:\n",
    "dataset_tokenized_text = np.array([\n",
    "    [word2index['<s>']] + [word2index[w] for w in s[1:]] + [word2index['</s>']] + [word2index['<pad>']]*(max_len-len(s))\n",
    "    for s in triplets\n",
    "    if word2index['<unk>'] not in [word2index[w] for w in s[1:]]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2254, 1238725)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_ferq), len(dataset_tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<s>', 'lady', 'has', 'ponytail', '</s>'],\n",
       " array([2252,    2,    3,    4, 2251]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab[w] for w in dataset_tokenized_text[0]],dataset_tokenized_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle and split the test/train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shuffle the index and then select from that\n",
    "X_index = np.arange(dataset_tokenized_text.shape[0])\n",
    "\n",
    "np.random.seed(int(np.pi*10001))\n",
    "np.random.shuffle(X_index)\n",
    "\n",
    "X = dataset_tokenized_text[X_index]\n",
    "\n",
    "test_split = int(X.shape[0]/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fold number 1 out of 10\n",
      "Number of groups of samples: 35\n",
      "Size of samples: 20\n",
      "Number of total samples: 700\n",
      "Epoch 1/40\n",
      "1114853/1114853 [==============================] - 27s - loss: 4.2094    \n",
      "Epoch 2/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.5292    \n",
      "Epoch 3/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.3830    \n",
      "Epoch 4/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.2874    \n",
      "Epoch 5/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.2152    \n",
      "Epoch 6/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.1690    \n",
      "Epoch 7/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.1372    \n",
      "Epoch 8/40\n",
      "1114853/1114853 [==============================] - 21s - loss: 3.1145    \n",
      "Epoch 9/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0971    \n",
      "Epoch 10/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0836    \n",
      "Epoch 11/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0727    \n",
      "Epoch 12/40\n",
      "1114853/1114853 [==============================] - 21s - loss: 3.0636    \n",
      "Epoch 13/40\n",
      "1114853/1114853 [==============================] - 21s - loss: 3.0559    \n",
      "Epoch 14/40\n",
      "1114853/1114853 [==============================] - 21s - loss: 3.0493    \n",
      "Epoch 15/40\n",
      "1114853/1114853 [==============================] - 23s - loss: 3.0435    \n",
      "Epoch 16/40\n",
      "1114853/1114853 [==============================] - 21s - loss: 3.0385    \n",
      "Epoch 17/40\n",
      "1114853/1114853 [==============================] - 21s - loss: 3.0339    \n",
      "Epoch 18/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0300    \n",
      "Epoch 19/40\n",
      "1114853/1114853 [==============================] - 21s - loss: 3.0263    \n",
      "Epoch 20/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0229    \n",
      "Epoch 21/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0199    \n",
      "Epoch 22/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0171    \n",
      "Epoch 23/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0144    \n",
      "Epoch 24/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0120    \n",
      "Epoch 25/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0098    \n",
      "Epoch 26/40\n",
      "1114853/1114853 [==============================] - 21s - loss: 3.0077    \n",
      "Epoch 27/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0057    \n",
      "Epoch 28/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0038    \n",
      "Epoch 29/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0021    \n",
      "Epoch 30/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0004    \n",
      "Epoch 31/40\n",
      "1114853/1114853 [==============================] - 21s - loss: 2.9989    \n",
      "Epoch 32/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9974    \n",
      "Epoch 33/40\n",
      "1114853/1114853 [==============================] - 21s - loss: 2.9960    \n",
      "Epoch 34/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9946    \n",
      "Epoch 35/40\n",
      "1114853/1114853 [==============================] - 21s - loss: 2.9934    \n",
      "Epoch 36/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9922    \n",
      "Epoch 37/40\n",
      "1114853/1114853 [==============================] - 21s - loss: 2.9910    \n",
      "Epoch 38/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9899    \n",
      "Epoch 39/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9888    \n",
      "Epoch 40/40\n",
      "1114853/1114853 [==============================] - 21s - loss: 2.9878    \n",
      "Epoch 1/40\n",
      "7000/7000 [==============================] - 3s - loss: 5.5165     \n",
      "Epoch 2/40\n",
      "7000/7000 [==============================] - 2s - loss: 4.4551     \n",
      "Epoch 3/40\n",
      "7000/7000 [==============================] - 1s - loss: 4.1877     \n",
      "Epoch 4/40\n",
      "7000/7000 [==============================] - 1s - loss: 4.0286     \n",
      "Epoch 5/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.9071     \n",
      "Epoch 6/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.8409     \n",
      "Epoch 7/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7971     \n",
      "Epoch 8/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.7603     \n",
      "Epoch 9/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.7274     \n",
      "Epoch 10/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.6974     \n",
      "Epoch 11/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6707     \n",
      "Epoch 12/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.6456     \n",
      "Epoch 13/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.6202     \n",
      "Epoch 14/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.5951     \n",
      "Epoch 15/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5713     \n",
      "Epoch 16/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.5464     \n",
      "Epoch 17/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5241     \n",
      "Epoch 18/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.5016     \n",
      "Epoch 19/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.4807     \n",
      "Epoch 20/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4606     \n",
      "Epoch 21/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4410     \n",
      "Epoch 22/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.4225     \n",
      "Epoch 23/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4034     \n",
      "Epoch 24/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.3853     \n",
      "Epoch 25/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3675     \n",
      "Epoch 26/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.3516     \n",
      "Epoch 27/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3351     \n",
      "Epoch 28/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.3193     \n",
      "Epoch 29/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3044     \n",
      "Epoch 30/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.2905     \n",
      "Epoch 31/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2774     \n",
      "Epoch 32/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.2651     \n",
      "Epoch 33/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.2531     \n",
      "Epoch 34/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.2416     \n",
      "Epoch 35/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.2307     \n",
      "Epoch 36/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.2195     \n",
      "Epoch 37/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.2086     \n",
      "Epoch 38/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.1981     \n",
      "Epoch 39/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.1876     \n",
      "Epoch 40/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.1775     \n",
      "The fold number 2 out of 10\n",
      "Number of groups of samples: 35\n",
      "Size of samples: 20\n",
      "Number of total samples: 700\n",
      "Epoch 1/40\n",
      "1114853/1114853 [==============================] - 21s - loss: 4.2147    \n",
      "Epoch 2/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.5385    \n",
      "Epoch 3/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.3872    \n",
      "Epoch 4/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.2826    \n",
      "Epoch 5/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.2162    \n",
      "Epoch 6/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.1721    \n",
      "Epoch 7/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.1414    \n",
      "Epoch 8/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.1186    \n",
      "Epoch 9/40\n",
      "1114853/1114853 [==============================] - 21s - loss: 3.1011    \n",
      "Epoch 10/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0872    \n",
      "Epoch 11/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0757    \n",
      "Epoch 12/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0662    \n",
      "Epoch 13/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0582    \n",
      "Epoch 14/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0515    \n",
      "Epoch 15/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0454    \n",
      "Epoch 16/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1114853/1114853 [==============================] - 20s - loss: 3.0402    \n",
      "Epoch 17/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0355    \n",
      "Epoch 18/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0314    \n",
      "Epoch 19/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0276    \n",
      "Epoch 20/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0241    \n",
      "Epoch 21/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0210    \n",
      "Epoch 22/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0181    \n",
      "Epoch 23/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0154    \n",
      "Epoch 24/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0129    \n",
      "Epoch 25/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0106    \n",
      "Epoch 26/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0085    \n",
      "Epoch 27/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0064    \n",
      "Epoch 28/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0045    \n",
      "Epoch 29/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0027    \n",
      "Epoch 30/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0009    \n",
      "Epoch 31/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9994    \n",
      "Epoch 32/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9978    \n",
      "Epoch 33/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9963    \n",
      "Epoch 34/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9950    \n",
      "Epoch 35/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9937    \n",
      "Epoch 36/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9924    \n",
      "Epoch 37/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9911    \n",
      "Epoch 38/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9900    \n",
      "Epoch 39/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9889    \n",
      "Epoch 40/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9878    \n",
      "Epoch 1/40\n",
      "7000/7000 [==============================] - 2s - loss: 5.5338     \n",
      "Epoch 2/40\n",
      "7000/7000 [==============================] - 1s - loss: 4.5306     \n",
      "Epoch 3/40\n",
      "7000/7000 [==============================] - 1s - loss: 4.2034     \n",
      "Epoch 4/40\n",
      "7000/7000 [==============================] - 1s - loss: 4.0142     \n",
      "Epoch 5/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.8884     \n",
      "Epoch 6/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.8294     \n",
      "Epoch 7/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.7952     \n",
      "Epoch 8/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7666     \n",
      "Epoch 9/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7412     \n",
      "Epoch 10/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7179     \n",
      "Epoch 11/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6975     \n",
      "Epoch 12/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6796     \n",
      "Epoch 13/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.6644     \n",
      "Epoch 14/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.6504     \n",
      "Epoch 15/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6361     \n",
      "Epoch 16/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6202     \n",
      "Epoch 17/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6029     \n",
      "Epoch 18/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5843     \n",
      "Epoch 19/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5667     \n",
      "Epoch 20/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5507     \n",
      "Epoch 21/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5347     \n",
      "Epoch 22/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.5180     \n",
      "Epoch 23/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5004     \n",
      "Epoch 24/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4819     \n",
      "Epoch 25/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4630     \n",
      "Epoch 26/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.4461     \n",
      "Epoch 27/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.4286     \n",
      "Epoch 28/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4116     \n",
      "Epoch 29/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3954     \n",
      "Epoch 30/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3808     \n",
      "Epoch 31/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3644     \n",
      "Epoch 32/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3480     \n",
      "Epoch 33/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3336     \n",
      "Epoch 34/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3191     \n",
      "Epoch 35/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3041     \n",
      "Epoch 36/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2904     \n",
      "Epoch 37/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2780     \n",
      "Epoch 38/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2650     \n",
      "Epoch 39/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2525     \n",
      "Epoch 40/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2405     \n",
      "The fold number 3 out of 10\n",
      "Number of groups of samples: 35\n",
      "Size of samples: 20\n",
      "Number of total samples: 700\n",
      "Epoch 1/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 4.2146    \n",
      "Epoch 2/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.5289    \n",
      "Epoch 3/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.3673    \n",
      "Epoch 4/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.2588    \n",
      "Epoch 5/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.1981    \n",
      "Epoch 6/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.1584    \n",
      "Epoch 7/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.1301    \n",
      "Epoch 8/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.1091    \n",
      "Epoch 9/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0933    \n",
      "Epoch 10/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0804    \n",
      "Epoch 11/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0700    \n",
      "Epoch 12/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0611    \n",
      "Epoch 13/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0535    \n",
      "Epoch 14/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0468    \n",
      "Epoch 15/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0411    \n",
      "Epoch 16/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0361    \n",
      "Epoch 17/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0317    \n",
      "Epoch 18/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0277    \n",
      "Epoch 19/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0241    \n",
      "Epoch 20/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0209    \n",
      "Epoch 21/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0180    \n",
      "Epoch 22/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0153    \n",
      "Epoch 23/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0129    \n",
      "Epoch 24/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0106    \n",
      "Epoch 25/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0085    \n",
      "Epoch 26/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0065    \n",
      "Epoch 27/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0046    \n",
      "Epoch 28/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0029    \n",
      "Epoch 29/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0012    \n",
      "Epoch 30/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9996    \n",
      "Epoch 31/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9982    \n",
      "Epoch 32/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9968    \n",
      "Epoch 33/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1114853/1114853 [==============================] - 20s - loss: 2.9955    \n",
      "Epoch 34/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9942    \n",
      "Epoch 35/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9931    \n",
      "Epoch 36/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9919    \n",
      "Epoch 37/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9908    \n",
      "Epoch 38/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9899    \n",
      "Epoch 39/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9888    \n",
      "Epoch 40/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9879    \n",
      "Epoch 1/40\n",
      "7000/7000 [==============================] - 3s - loss: 5.5222     \n",
      "Epoch 2/40\n",
      "7000/7000 [==============================] - 1s - loss: 4.4891     \n",
      "Epoch 3/40\n",
      "7000/7000 [==============================] - 1s - loss: 4.1855     \n",
      "Epoch 4/40\n",
      "7000/7000 [==============================] - 1s - loss: 4.0299     \n",
      "Epoch 5/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.9144     \n",
      "Epoch 6/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.8455     \n",
      "Epoch 7/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.8043     \n",
      "Epoch 8/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7669     \n",
      "Epoch 9/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7314     \n",
      "Epoch 10/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6962     \n",
      "Epoch 11/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6667     \n",
      "Epoch 12/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6388     \n",
      "Epoch 13/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.6141     \n",
      "Epoch 14/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5892     \n",
      "Epoch 15/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5662     \n",
      "Epoch 16/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5430     \n",
      "Epoch 17/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.5195     \n",
      "Epoch 18/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4969     \n",
      "Epoch 19/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4759     \n",
      "Epoch 20/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4546     \n",
      "Epoch 21/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4334     \n",
      "Epoch 22/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4127     \n",
      "Epoch 23/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.3922     \n",
      "Epoch 24/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.3729     \n",
      "Epoch 25/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3543     \n",
      "Epoch 26/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.3362     \n",
      "Epoch 27/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3192     \n",
      "Epoch 28/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.3032     \n",
      "Epoch 29/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2872     \n",
      "Epoch 30/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2729     \n",
      "Epoch 31/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2580     \n",
      "Epoch 32/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2439     \n",
      "Epoch 33/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2298     \n",
      "Epoch 34/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.2181     \n",
      "Epoch 35/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2045     \n",
      "Epoch 36/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.1920     \n",
      "Epoch 37/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.1797     \n",
      "Epoch 38/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.1680     \n",
      "Epoch 39/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.1570     \n",
      "Epoch 40/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.1455     \n",
      "The fold number 4 out of 10\n",
      "Number of groups of samples: 35\n",
      "Size of samples: 20\n",
      "Number of total samples: 700\n",
      "Epoch 1/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 4.2238    \n",
      "Epoch 2/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.5138    \n",
      "Epoch 3/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.3638    \n",
      "Epoch 4/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.2689    \n",
      "Epoch 5/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.2055    \n",
      "Epoch 6/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.1649    \n",
      "Epoch 7/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.1374    \n",
      "Epoch 8/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.1168    \n",
      "Epoch 9/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.1002    \n",
      "Epoch 10/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0860    \n",
      "Epoch 11/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0743    \n",
      "Epoch 12/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0645    \n",
      "Epoch 13/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0564    \n",
      "Epoch 14/40\n",
      "1114853/1114853 [==============================] - 21s - loss: 3.0494    \n",
      "Epoch 15/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0434    \n",
      "Epoch 16/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0380    \n",
      "Epoch 17/40\n",
      "1114853/1114853 [==============================] - 22s - loss: 3.0333    \n",
      "Epoch 18/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0290    \n",
      "Epoch 19/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.0251    \n",
      "Epoch 20/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0217    \n",
      "Epoch 21/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0185    \n",
      "Epoch 22/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0156    \n",
      "Epoch 23/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0130    \n",
      "Epoch 24/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0105    \n",
      "Epoch 25/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0082    \n",
      "Epoch 26/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0061    \n",
      "Epoch 27/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0041    \n",
      "Epoch 28/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0022    \n",
      "Epoch 29/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0004    \n",
      "Epoch 30/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9988    \n",
      "Epoch 31/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9972    \n",
      "Epoch 32/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9958    \n",
      "Epoch 33/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9944    \n",
      "Epoch 34/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9931    \n",
      "Epoch 35/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9918    \n",
      "Epoch 36/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9906    \n",
      "Epoch 37/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9895    \n",
      "Epoch 38/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9884    \n",
      "Epoch 39/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9874    \n",
      "Epoch 40/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9864    \n",
      "Epoch 1/40\n",
      "7000/7000 [==============================] - 2s - loss: 5.5166     \n",
      "Epoch 2/40\n",
      "7000/7000 [==============================] - 1s - loss: 4.5184     \n",
      "Epoch 3/40\n",
      "7000/7000 [==============================] - 1s - loss: 4.2009     \n",
      "Epoch 4/40\n",
      "7000/7000 [==============================] - 1s - loss: 4.0268     \n",
      "Epoch 5/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.9057     \n",
      "Epoch 6/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.8437     \n",
      "Epoch 7/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.8036     \n",
      "Epoch 8/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7722     \n",
      "Epoch 9/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7449     \n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 1s - loss: 3.7233     \n",
      "Epoch 11/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7041     \n",
      "Epoch 12/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6881     \n",
      "Epoch 13/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6725     \n",
      "Epoch 14/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6569     \n",
      "Epoch 15/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6380     \n",
      "Epoch 16/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6181     \n",
      "Epoch 17/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5962     \n",
      "Epoch 18/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5741     \n",
      "Epoch 19/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5519     \n",
      "Epoch 20/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5310     \n",
      "Epoch 21/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5099     \n",
      "Epoch 22/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4902     \n",
      "Epoch 23/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4722     \n",
      "Epoch 24/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4543     \n",
      "Epoch 25/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4379     \n",
      "Epoch 26/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4214     \n",
      "Epoch 27/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4063     \n",
      "Epoch 28/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3912     \n",
      "Epoch 29/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3769     \n",
      "Epoch 30/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3639     \n",
      "Epoch 31/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3496     \n",
      "Epoch 32/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3355     \n",
      "Epoch 33/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3221     \n",
      "Epoch 34/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3087     \n",
      "Epoch 35/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2969     \n",
      "Epoch 36/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2839     \n",
      "Epoch 37/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2712     \n",
      "Epoch 38/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2593     \n",
      "Epoch 39/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2467     \n",
      "Epoch 40/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2354     \n",
      "The fold number 5 out of 10\n",
      "Number of groups of samples: 35\n",
      "Size of samples: 20\n",
      "Number of total samples: 700\n",
      "Epoch 1/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 4.2309    \n",
      "Epoch 2/40\n",
      "1114853/1114853 [==============================] - 18s - loss: 3.5461    \n",
      "Epoch 3/40\n",
      "1114853/1114853 [==============================] - 18s - loss: 3.3641    \n",
      "Epoch 4/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.2627    \n",
      "Epoch 5/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.2024    \n",
      "Epoch 6/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.1639    \n",
      "Epoch 7/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.1366    \n",
      "Epoch 8/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.1155    \n",
      "Epoch 9/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0991    \n",
      "Epoch 10/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0861    \n",
      "Epoch 11/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0754    \n",
      "Epoch 12/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0664    \n",
      "Epoch 13/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0588    \n",
      "Epoch 14/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0522    \n",
      "Epoch 15/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0463    \n",
      "Epoch 16/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0411    \n",
      "Epoch 17/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0365    \n",
      "Epoch 18/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0323    \n",
      "Epoch 19/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0285    \n",
      "Epoch 20/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0251    \n",
      "Epoch 21/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0220    \n",
      "Epoch 22/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0190    \n",
      "Epoch 23/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0164    \n",
      "Epoch 24/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0140    \n",
      "Epoch 25/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0116    \n",
      "Epoch 26/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0096    \n",
      "Epoch 27/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0076    \n",
      "Epoch 28/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0056    \n",
      "Epoch 29/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0038    \n",
      "Epoch 30/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0021    \n",
      "Epoch 31/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0006    \n",
      "Epoch 32/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9990    \n",
      "Epoch 33/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9977    \n",
      "Epoch 34/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9962    \n",
      "Epoch 35/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9949    \n",
      "Epoch 36/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9936    \n",
      "Epoch 37/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9924    \n",
      "Epoch 38/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9913    \n",
      "Epoch 39/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9901    \n",
      "Epoch 40/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9891    \n",
      "Epoch 1/40\n",
      "7000/7000 [==============================] - 3s - loss: 5.5078     \n",
      "Epoch 2/40\n",
      "7000/7000 [==============================] - 1s - loss: 4.4245     \n",
      "Epoch 3/40\n",
      "7000/7000 [==============================] - 1s - loss: 4.1700     \n",
      "Epoch 4/40\n",
      "7000/7000 [==============================] - 1s - loss: 4.0237     \n",
      "Epoch 5/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.9063     \n",
      "Epoch 6/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.8497     \n",
      "Epoch 7/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.8123     \n",
      "Epoch 8/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7765     \n",
      "Epoch 9/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7386     \n",
      "Epoch 10/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7053     \n",
      "Epoch 11/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6744     \n",
      "Epoch 12/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6458     \n",
      "Epoch 13/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.6187     \n",
      "Epoch 14/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5930     \n",
      "Epoch 15/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5692     \n",
      "Epoch 16/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.5469     \n",
      "Epoch 17/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5251     \n",
      "Epoch 18/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5053     \n",
      "Epoch 19/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4856     \n",
      "Epoch 20/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4664     \n",
      "Epoch 21/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4468     \n",
      "Epoch 22/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4295     \n",
      "Epoch 23/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4119     \n",
      "Epoch 24/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3946     \n",
      "Epoch 25/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3782     \n",
      "Epoch 26/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3622     \n",
      "Epoch 27/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.3466     \n",
      "Epoch 28/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 1s - loss: 3.3303     \n",
      "Epoch 29/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3147     \n",
      "Epoch 30/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2992     \n",
      "Epoch 31/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2851     \n",
      "Epoch 32/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2706     \n",
      "Epoch 33/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2567     \n",
      "Epoch 34/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2430     \n",
      "Epoch 35/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2286     \n",
      "Epoch 36/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2162     \n",
      "Epoch 37/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2025     \n",
      "Epoch 38/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.1898     \n",
      "Epoch 39/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.1767     \n",
      "Epoch 40/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.1645     \n",
      "The fold number 6 out of 10\n",
      "Number of groups of samples: 35\n",
      "Size of samples: 20\n",
      "Number of total samples: 700\n",
      "Epoch 1/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 4.2379    \n",
      "Epoch 2/40\n",
      "1114853/1114853 [==============================] - 18s - loss: 3.5209    \n",
      "Epoch 3/40\n",
      "1114853/1114853 [==============================] - 18s - loss: 3.3449    \n",
      "Epoch 4/40\n",
      "1114853/1114853 [==============================] - 18s - loss: 3.2520    \n",
      "Epoch 5/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.1919    \n",
      "Epoch 6/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.1519    \n",
      "Epoch 7/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.1244    \n",
      "Epoch 8/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.1045    \n",
      "Epoch 9/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0889    \n",
      "Epoch 10/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0763    \n",
      "Epoch 11/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0656    \n",
      "Epoch 12/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0565    \n",
      "Epoch 13/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0490    \n",
      "Epoch 14/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0426    \n",
      "Epoch 15/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0372    \n",
      "Epoch 16/40\n",
      "1114853/1114853 [==============================] - 21s - loss: 3.0324    \n",
      "Epoch 17/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0280    \n",
      "Epoch 18/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0242    \n",
      "Epoch 19/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0207    \n",
      "Epoch 20/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0175    \n",
      "Epoch 21/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0147    \n",
      "Epoch 22/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0120    \n",
      "Epoch 23/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0096    \n",
      "Epoch 24/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0074    \n",
      "Epoch 25/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0052    \n",
      "Epoch 26/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0034    \n",
      "Epoch 27/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0016    \n",
      "Epoch 28/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9998    \n",
      "Epoch 29/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9982    \n",
      "Epoch 30/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9966    \n",
      "Epoch 31/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9952    \n",
      "Epoch 32/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9938    \n",
      "Epoch 33/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9926    \n",
      "Epoch 34/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9913    \n",
      "Epoch 35/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9901    \n",
      "Epoch 36/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9890    \n",
      "Epoch 37/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9880    \n",
      "Epoch 38/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9869    \n",
      "Epoch 39/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9859    \n",
      "Epoch 40/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9850    \n",
      "Epoch 1/40\n",
      "7000/7000 [==============================] - 3s - loss: 5.5434     \n",
      "Epoch 2/40\n",
      "7000/7000 [==============================] - 1s - loss: 4.4547     \n",
      "Epoch 3/40\n",
      "7000/7000 [==============================] - 1s - loss: 4.1408     \n",
      "Epoch 4/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.9826     \n",
      "Epoch 5/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.8889     \n",
      "Epoch 6/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.8390     \n",
      "Epoch 7/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7997     \n",
      "Epoch 8/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7647     \n",
      "Epoch 9/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7336     \n",
      "Epoch 10/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7050     \n",
      "Epoch 11/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6792     \n",
      "Epoch 12/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6541     \n",
      "Epoch 13/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6305     \n",
      "Epoch 14/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6076     \n",
      "Epoch 15/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5854     \n",
      "Epoch 16/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5650     \n",
      "Epoch 17/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5458     \n",
      "Epoch 18/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5263     \n",
      "Epoch 19/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5069     \n",
      "Epoch 20/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4862     \n",
      "Epoch 21/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4659     \n",
      "Epoch 22/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4451     \n",
      "Epoch 23/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4237     \n",
      "Epoch 24/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4038     \n",
      "Epoch 25/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3847     \n",
      "Epoch 26/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3667     \n",
      "Epoch 27/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3489     \n",
      "Epoch 28/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3326     \n",
      "Epoch 29/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3167     \n",
      "Epoch 30/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3006     \n",
      "Epoch 31/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2869     \n",
      "Epoch 32/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2721     \n",
      "Epoch 33/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2586     \n",
      "Epoch 34/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2454     \n",
      "Epoch 35/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2324     \n",
      "Epoch 36/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2201     \n",
      "Epoch 37/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2075     \n",
      "Epoch 38/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.1959     \n",
      "Epoch 39/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.1839     \n",
      "Epoch 40/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.1722     \n",
      "The fold number 7 out of 10\n",
      "Number of groups of samples: 35\n",
      "Size of samples: 20\n",
      "Number of total samples: 700\n",
      "Epoch 1/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 4.2040    \n",
      "Epoch 2/40\n",
      "1114853/1114853 [==============================] - 18s - loss: 3.4956    \n",
      "Epoch 3/40\n",
      "1114853/1114853 [==============================] - 18s - loss: 3.3158    \n",
      "Epoch 4/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1114853/1114853 [==============================] - 20s - loss: 3.2366    \n",
      "Epoch 5/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.1911    \n",
      "Epoch 6/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.1586    \n",
      "Epoch 7/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.1335    \n",
      "Epoch 8/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.1142    \n",
      "Epoch 9/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0985    \n",
      "Epoch 10/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0861    \n",
      "Epoch 11/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0758    \n",
      "Epoch 12/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0672    \n",
      "Epoch 13/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0597    \n",
      "Epoch 14/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0530    \n",
      "Epoch 15/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0470    \n",
      "Epoch 16/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0416    \n",
      "Epoch 17/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0367    \n",
      "Epoch 18/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0325    \n",
      "Epoch 19/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0287    \n",
      "Epoch 20/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0251    \n",
      "Epoch 21/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0219    \n",
      "Epoch 22/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0190    \n",
      "Epoch 23/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0163    \n",
      "Epoch 24/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0138    \n",
      "Epoch 25/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0114    \n",
      "Epoch 26/40\n",
      "1114853/1114853 [==============================] - 21s - loss: 3.0093    \n",
      "Epoch 27/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0072    \n",
      "Epoch 28/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0053    \n",
      "Epoch 29/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0034    \n",
      "Epoch 30/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0017    \n",
      "Epoch 31/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0001    \n",
      "Epoch 32/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9986    \n",
      "Epoch 33/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9972    \n",
      "Epoch 34/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9957    \n",
      "Epoch 35/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9944    \n",
      "Epoch 36/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9932    \n",
      "Epoch 37/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9921    \n",
      "Epoch 38/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9909    \n",
      "Epoch 39/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9898    \n",
      "Epoch 40/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9888    \n",
      "Epoch 1/40\n",
      "7000/7000 [==============================] - 3s - loss: 5.5287     \n",
      "Epoch 2/40\n",
      "7000/7000 [==============================] - 1s - loss: 4.4512     \n",
      "Epoch 3/40\n",
      "7000/7000 [==============================] - 1s - loss: 4.1276     \n",
      "Epoch 4/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.9439     \n",
      "Epoch 5/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.8724     \n",
      "Epoch 6/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.8313     \n",
      "Epoch 7/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7964     \n",
      "Epoch 8/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7628     \n",
      "Epoch 9/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.7318     \n",
      "Epoch 10/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7005     \n",
      "Epoch 11/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6714     \n",
      "Epoch 12/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6436     \n",
      "Epoch 13/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6188     \n",
      "Epoch 14/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5973     \n",
      "Epoch 15/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5750     \n",
      "Epoch 16/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5543     \n",
      "Epoch 17/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5347     \n",
      "Epoch 18/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5161     \n",
      "Epoch 19/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4981     \n",
      "Epoch 20/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4803     \n",
      "Epoch 21/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.4641     \n",
      "Epoch 22/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4480     \n",
      "Epoch 23/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4317     \n",
      "Epoch 24/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4167     \n",
      "Epoch 25/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4009     \n",
      "Epoch 26/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3854     \n",
      "Epoch 27/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3698     \n",
      "Epoch 28/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3546     \n",
      "Epoch 29/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3401     \n",
      "Epoch 30/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3258     \n",
      "Epoch 31/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3112     \n",
      "Epoch 32/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2972     \n",
      "Epoch 33/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2836     \n",
      "Epoch 34/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2696     \n",
      "Epoch 35/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.2561     \n",
      "Epoch 36/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2436     \n",
      "Epoch 37/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2311     \n",
      "Epoch 38/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2191     \n",
      "Epoch 39/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2066     \n",
      "Epoch 40/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.1950     \n",
      "The fold number 8 out of 10\n",
      "Number of groups of samples: 35\n",
      "Size of samples: 20\n",
      "Number of total samples: 700\n",
      "Epoch 1/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 4.2088    \n",
      "Epoch 2/40\n",
      "1114853/1114853 [==============================] - 18s - loss: 3.5173    \n",
      "Epoch 3/40\n",
      "1114853/1114853 [==============================] - 18s - loss: 3.3470    \n",
      "Epoch 4/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.2516    \n",
      "Epoch 5/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.1952    \n",
      "Epoch 6/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.1593    \n",
      "Epoch 7/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.1334    \n",
      "Epoch 8/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.1136    \n",
      "Epoch 9/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.0982    \n",
      "Epoch 10/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.0860    \n",
      "Epoch 11/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.0759    \n",
      "Epoch 12/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0673    \n",
      "Epoch 13/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0601    \n",
      "Epoch 14/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.0538    \n",
      "Epoch 15/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.0483    \n",
      "Epoch 16/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.0433    \n",
      "Epoch 17/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0389    \n",
      "Epoch 18/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0350    \n",
      "Epoch 19/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0314    \n",
      "Epoch 20/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.0281    \n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1114853/1114853 [==============================] - 19s - loss: 3.0250    \n",
      "Epoch 22/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.0223    \n",
      "Epoch 23/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0195    \n",
      "Epoch 24/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0170    \n",
      "Epoch 25/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.0147    \n",
      "Epoch 26/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0125    \n",
      "Epoch 27/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0105    \n",
      "Epoch 28/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0085    \n",
      "Epoch 29/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0067    \n",
      "Epoch 30/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0049    \n",
      "Epoch 31/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.0033    \n",
      "Epoch 32/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0018    \n",
      "Epoch 33/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.0003    \n",
      "Epoch 34/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9989    \n",
      "Epoch 35/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9976    \n",
      "Epoch 36/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9963    \n",
      "Epoch 37/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9950    \n",
      "Epoch 38/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9939    \n",
      "Epoch 39/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9928    \n",
      "Epoch 40/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9917    \n",
      "Epoch 1/40\n",
      "7000/7000 [==============================] - 3s - loss: 5.5486     \n",
      "Epoch 2/40\n",
      "7000/7000 [==============================] - 1s - loss: 4.4931     \n",
      "Epoch 3/40\n",
      "7000/7000 [==============================] - 1s - loss: 4.1590     \n",
      "Epoch 4/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.9879     \n",
      "Epoch 5/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.8821     \n",
      "Epoch 6/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.8327     \n",
      "Epoch 7/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7965     \n",
      "Epoch 8/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7641     \n",
      "Epoch 9/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7358     \n",
      "Epoch 10/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7116     \n",
      "Epoch 11/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6916     \n",
      "Epoch 12/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6732     \n",
      "Epoch 13/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6538     \n",
      "Epoch 14/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6340     \n",
      "Epoch 15/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6147     \n",
      "Epoch 16/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5969     \n",
      "Epoch 17/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5780     \n",
      "Epoch 18/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5605     \n",
      "Epoch 19/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5430     \n",
      "Epoch 20/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5253     \n",
      "Epoch 21/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5097     \n",
      "Epoch 22/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4936     \n",
      "Epoch 23/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4787     \n",
      "Epoch 24/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4634     \n",
      "Epoch 25/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4483     \n",
      "Epoch 26/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4323     \n",
      "Epoch 27/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4179     \n",
      "Epoch 28/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4034     \n",
      "Epoch 29/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3878     \n",
      "Epoch 30/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3728     \n",
      "Epoch 31/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3573     \n",
      "Epoch 32/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3423     \n",
      "Epoch 33/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3277     \n",
      "Epoch 34/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3134     \n",
      "Epoch 35/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2997     \n",
      "Epoch 36/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2853     \n",
      "Epoch 37/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2709     \n",
      "Epoch 38/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2573     \n",
      "Epoch 39/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2439     \n",
      "Epoch 40/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2316     \n",
      "The fold number 9 out of 10\n",
      "Number of groups of samples: 34\n",
      "Size of samples: 20\n",
      "Number of total samples: 680\n",
      "Epoch 1/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 4.2546    \n",
      "Epoch 2/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.5384    \n",
      "Epoch 3/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.3651    \n",
      "Epoch 4/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.2705    \n",
      "Epoch 5/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.2060    \n",
      "Epoch 6/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.1638    \n",
      "Epoch 7/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.1347    \n",
      "Epoch 8/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.1129    \n",
      "Epoch 9/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0962    \n",
      "Epoch 10/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0834    \n",
      "Epoch 11/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0729    \n",
      "Epoch 12/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0641    \n",
      "Epoch 13/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0567    \n",
      "Epoch 14/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0502    \n",
      "Epoch 15/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0445    \n",
      "Epoch 16/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0393    \n",
      "Epoch 17/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0348    \n",
      "Epoch 18/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0308    \n",
      "Epoch 19/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0271    \n",
      "Epoch 20/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0237    \n",
      "Epoch 21/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0205    \n",
      "Epoch 22/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0177    \n",
      "Epoch 23/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0151    \n",
      "Epoch 24/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0126    \n",
      "Epoch 25/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0104    \n",
      "Epoch 26/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0082    \n",
      "Epoch 27/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0062    \n",
      "Epoch 28/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0043    \n",
      "Epoch 29/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0025    \n",
      "Epoch 30/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0009    \n",
      "Epoch 31/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9993    \n",
      "Epoch 32/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9978    \n",
      "Epoch 33/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9964    \n",
      "Epoch 34/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9951    \n",
      "Epoch 35/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9937    \n",
      "Epoch 36/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9925    \n",
      "Epoch 37/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9914    \n",
      "Epoch 38/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1114853/1114853 [==============================] - 20s - loss: 2.9903    \n",
      "Epoch 39/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9892    \n",
      "Epoch 40/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9882    \n",
      "Epoch 1/40\n",
      "7000/7000 [==============================] - 7s - loss: 5.5535     \n",
      "Epoch 2/40\n",
      "7000/7000 [==============================] - 1s - loss: 4.4599     \n",
      "Epoch 3/40\n",
      "7000/7000 [==============================] - 1s - loss: 4.1469     \n",
      "Epoch 4/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.9867     \n",
      "Epoch 5/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.8838     \n",
      "Epoch 6/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.8293     \n",
      "Epoch 7/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7868     \n",
      "Epoch 8/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.7494     \n",
      "Epoch 9/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7162     \n",
      "Epoch 10/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6877     \n",
      "Epoch 11/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6623     \n",
      "Epoch 12/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6401     \n",
      "Epoch 13/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6195     \n",
      "Epoch 14/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5996     \n",
      "Epoch 15/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.5791     \n",
      "Epoch 16/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5574     \n",
      "Epoch 17/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.5354     \n",
      "Epoch 18/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5123     \n",
      "Epoch 19/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.4888     \n",
      "Epoch 20/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4683     \n",
      "Epoch 21/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4493     \n",
      "Epoch 22/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.4309     \n",
      "Epoch 23/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4141     \n",
      "Epoch 24/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.3984     \n",
      "Epoch 25/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3826     \n",
      "Epoch 26/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3679     \n",
      "Epoch 27/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.3530     \n",
      "Epoch 28/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3383     \n",
      "Epoch 29/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3245     \n",
      "Epoch 30/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3102     \n",
      "Epoch 31/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2966     \n",
      "Epoch 32/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2835     \n",
      "Epoch 33/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2706     \n",
      "Epoch 34/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2581     \n",
      "Epoch 35/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2451     \n",
      "Epoch 36/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2333     \n",
      "Epoch 37/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2216     \n",
      "Epoch 38/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.2100     \n",
      "Epoch 39/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.1983     \n",
      "Epoch 40/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.1860     \n",
      "The fold number 10 out of 10\n",
      "Number of groups of samples: 35\n",
      "Size of samples: 20\n",
      "Number of total samples: 700\n",
      "Epoch 1/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 4.2030    \n",
      "Epoch 2/40\n",
      "1114853/1114853 [==============================] - 18s - loss: 3.5152    \n",
      "Epoch 3/40\n",
      "1114853/1114853 [==============================] - 18s - loss: 3.3717    \n",
      "Epoch 4/40\n",
      "1114853/1114853 [==============================] - 18s - loss: 3.2747    \n",
      "Epoch 5/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.2096    \n",
      "Epoch 6/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.1649    \n",
      "Epoch 7/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.1349    \n",
      "Epoch 8/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.1127    \n",
      "Epoch 9/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.0956    \n",
      "Epoch 10/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.0824    \n",
      "Epoch 11/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.0719    \n",
      "Epoch 12/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.0633    \n",
      "Epoch 13/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.0560    \n",
      "Epoch 14/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.0496    \n",
      "Epoch 15/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0441    \n",
      "Epoch 16/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.0392    \n",
      "Epoch 17/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0349    \n",
      "Epoch 18/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0310    \n",
      "Epoch 19/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0274    \n",
      "Epoch 20/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0243    \n",
      "Epoch 21/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0214    \n",
      "Epoch 22/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0187    \n",
      "Epoch 23/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0162    \n",
      "Epoch 24/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0139    \n",
      "Epoch 25/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.0117    \n",
      "Epoch 26/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0097    \n",
      "Epoch 27/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 3.0078    \n",
      "Epoch 28/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0060    \n",
      "Epoch 29/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0042    \n",
      "Epoch 30/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0027    \n",
      "Epoch 31/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 3.0011    \n",
      "Epoch 32/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9997    \n",
      "Epoch 33/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9983    \n",
      "Epoch 34/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9970    \n",
      "Epoch 35/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 2.9958    \n",
      "Epoch 36/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9945    \n",
      "Epoch 37/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9934    \n",
      "Epoch 38/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 2.9922    \n",
      "Epoch 39/40\n",
      "1114853/1114853 [==============================] - 19s - loss: 2.9912    \n",
      "Epoch 40/40\n",
      "1114853/1114853 [==============================] - 20s - loss: 2.9901    \n",
      "Epoch 1/40\n",
      "7000/7000 [==============================] - 3s - loss: 5.5589     \n",
      "Epoch 2/40\n",
      "7000/7000 [==============================] - 2s - loss: 4.4866     \n",
      "Epoch 3/40\n",
      "7000/7000 [==============================] - 1s - loss: 4.1663     \n",
      "Epoch 4/40\n",
      "7000/7000 [==============================] - 1s - loss: 4.0139     \n",
      "Epoch 5/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.9108     \n",
      "Epoch 6/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.8408     \n",
      "Epoch 7/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.7995     \n",
      "Epoch 8/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.7657     \n",
      "Epoch 9/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.7323     \n",
      "Epoch 10/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.7049     \n",
      "Epoch 11/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.6805     \n",
      "Epoch 12/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6596     \n",
      "Epoch 13/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6409     \n",
      "Epoch 14/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.6222     \n",
      "Epoch 15/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.6035     \n",
      "Epoch 16/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 2s - loss: 3.5849     \n",
      "Epoch 17/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.5658     \n",
      "Epoch 18/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.5461     \n",
      "Epoch 19/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.5245     \n",
      "Epoch 20/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.5023     \n",
      "Epoch 21/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4811     \n",
      "Epoch 22/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4597     \n",
      "Epoch 23/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4396     \n",
      "Epoch 24/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.4191     \n",
      "Epoch 25/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.4014     \n",
      "Epoch 26/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3849     \n",
      "Epoch 27/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.3675     \n",
      "Epoch 28/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3514     \n",
      "Epoch 29/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3362     \n",
      "Epoch 30/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3208     \n",
      "Epoch 31/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.3058     \n",
      "Epoch 32/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.2922     \n",
      "Epoch 33/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2775     \n",
      "Epoch 34/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2640     \n",
      "Epoch 35/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2506     \n",
      "Epoch 36/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.2377     \n",
      "Epoch 37/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.2252     \n",
      "Epoch 38/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2134     \n",
      "Epoch 39/40\n",
      "7000/7000 [==============================] - 1s - loss: 3.2015     \n",
      "Epoch 40/40\n",
      "7000/7000 [==============================] - 2s - loss: 3.1897     \n"
     ]
    }
   ],
   "source": [
    "for cv in range(10):\n",
    "    print(\"The fold number {} out of 10\".format(cv+1))\n",
    "    X_test  = X[cv*test_split:(cv+1)*test_split]\n",
    "    X_train = X[list(range(0,cv*test_split))+list(range((cv+1)*test_split,X.shape[0]))]\n",
    "    \n",
    "    ######## Create a downsampled training data ########\n",
    "    # Available relations \n",
    "    avalable_relations = [word2index[rel] for rel in composit2simple.values() if word2index[rel]!=vocab.index('<unk>')]\n",
    "\n",
    "    # buckets:\n",
    "    bucket_size = 200\n",
    "    X_train2_index_rels = {\n",
    "        rel: []\n",
    "        for rel in avalable_relations\n",
    "    }\n",
    "\n",
    "    # Fill the buckets:\n",
    "    for i, sent in enumerate(X_train):\n",
    "        if sent[2] in X_train2_index_rels and len(X_train2_index_rels[sent[2]]) < bucket_size:\n",
    "            X_train2_index_rels[sent[2]].append(i)\n",
    "\n",
    "    # Do not use small buckets at all:\n",
    "    X_train2_index = np.concatenate([\n",
    "        X_train2_index_rels[rel]\n",
    "        for rel in X_train2_index_rels\n",
    "        if len(X_train2_index_rels[rel]) == bucket_size\n",
    "    ])\n",
    "\n",
    "    # Here is the final training data for balanced training:\n",
    "    X_train2 = X_train[X_train2_index]\n",
    "    \n",
    "    ####### the vocabulary changes for balanced training data  ########\n",
    "    # this code is not optimised but it works.\n",
    "\n",
    "    # Shared vocabulary between two trainin set\n",
    "    new_vocab_index_freq = Counter(w for s in X_train2 for w in s)\n",
    "    new_vocab_index = list(new_vocab_index_freq.keys())\n",
    "\n",
    "    # Spatial vocabulary\n",
    "    spl_vocab = list(set([w for w in composit2simple.values() if w in vocab]))\n",
    "    spl_vocab_index = [word2index[w] for w in spl_vocab if word2index[w] in new_vocab_index]\n",
    "    \n",
    "    \n",
    "    ########## Create test collections! ########\n",
    "    sample_bucket_size = 20\n",
    "\n",
    "    # initialise the buckets:\n",
    "    X_test2_index_rels = {\n",
    "        rel: []\n",
    "        for rel in spl_vocab_index\n",
    "    }\n",
    "\n",
    "    # put each sample in its bucket:\n",
    "    for i, sent in enumerate(X_test):\n",
    "        if sent[2] in X_test2_index_rels and \\\n",
    "            len(X_test2_index_rels[sent[2]]) < sample_bucket_size and \\\n",
    "            len([w for w in sent if w not in new_vocab_index])==0:\n",
    "            X_test2_index_rels[sent[2]].append(i)\n",
    "\n",
    "\n",
    "    valid_test_rels = [rel for rel in X_test2_index_rels if new_vocab_index_freq[rel] >= 200 and len(X_test2_index_rels[rel]) == sample_bucket_size]\n",
    "    # alphabetic sort sort\n",
    "    valid_test_rels = [rel for _, rel in sorted([(vocab[r], r) for r in valid_test_rels])]\n",
    "\n",
    "    X_test2_index = np.concatenate([\n",
    "        X_test2_index_rels[rel]\n",
    "        for rel in X_test2_index_rels\n",
    "        if rel in valid_test_rels\n",
    "        if len(X_test2_index_rels[rel]) == sample_bucket_size\n",
    "    ])\n",
    "\n",
    "    X_test2 = X_test[X_test2_index]\n",
    "\n",
    "    print('Number of groups of samples:', len(valid_test_rels))\n",
    "    print('Size of samples:', sample_bucket_size)\n",
    "    print('Number of total samples:', len(X_test2))\n",
    "    \n",
    "    \n",
    "    ######## Training the language model ########\n",
    "    # some data/model hyperparameters:\n",
    "    max_len = max_len\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_size = 300\n",
    "    memory_size = 300\n",
    "    \n",
    "    # training model 1\n",
    "    lm1 = build_model(\n",
    "        max_len = max_len,\n",
    "        vocab_size = vocab_size,\n",
    "    )\n",
    "    lm1.fit(X_train[:, :-1], np.expand_dims(X_train[:, 1:], 2), epochs=40, batch_size=1024)\n",
    "    \n",
    "    # training model 2\n",
    "    lm2 = build_model(\n",
    "        max_len = max_len,\n",
    "        vocab_size = vocab_size,\n",
    "    )\n",
    "    lm2.fit(X_train2[:, :-1], np.expand_dims(X_train2[:, 1:], 2), epochs=40, batch_size=32)\n",
    "    \n",
    "    \n",
    "    ####### reporting procedures #######\n",
    "    \n",
    "    # these functions could be optimised, in this shape they are using some global variables ...\n",
    "    def report(lm, X_test, rels):\n",
    "        X_test_spl = [\n",
    "            [w for w in sent if w in rels][0]\n",
    "            for sent in X_test\n",
    "        ]\n",
    "\n",
    "        results_sum= defaultdict(int)\n",
    "        results_count= defaultdict(float)\n",
    "        results_sum_x = defaultdict(lambda: np.zeros(len(rels)))\n",
    "\n",
    "        X_test_X = [\n",
    "            np.array([[X_test[i][0],X_test[i][1], r, X_test[i][3], X_test[i][4]] for r in rels])\n",
    "            for i,rel in enumerate(X_test_spl)\n",
    "        ]\n",
    "\n",
    "        for term, pp, xchanges in zip(X_test_spl, list(perplexity(lm, X_test).flatten()), X_test_X):\n",
    "            results_sum_x[term] += np.log2(perplexity(lm, xchanges).flatten())\n",
    "            results_sum[term] += np.log2(pp)\n",
    "            results_count[term] += 1\n",
    "\n",
    "        results = [\n",
    "            [vocab[term], 2**(results_sum[term]/results_count[term]), results_count[term]] + \\\n",
    "            [2**(results_sum_x[term][r]/results_count[term]) for r in range(len(rels))]\n",
    "            for term in results_sum\n",
    "        ]\n",
    "\n",
    "        #results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return(results)\n",
    "\n",
    "    def report_to_string(results, rels):\n",
    "        output = ''\n",
    "        output += (\"{}\\t{}\\t{}\"+\"\\t{}\"*len(rels)).format(*(['term', 'sample_pp', 'sample_size']+[vocab[rel] for rel in rels])) + '\\n'\n",
    "        for x in results:\n",
    "            output += (\"{}\\t{:.2f}\\t{}\"+\"\\t{:.2f}\"*len(rels)).format(*x) + '\\n'\n",
    "\n",
    "        return output\n",
    "    \n",
    "    ######### generate the reports #########\n",
    "    r = report(lm1, X_test2, valid_test_rels)\n",
    "\n",
    "    r_voc = [\n",
    "        word2index[rrr[0]]\n",
    "        for rrr in r\n",
    "    ]\n",
    "    r_ = np.array([\n",
    "        rrr[3:]\n",
    "        for rrr in r\n",
    "    ])\n",
    "\n",
    "    # fix the order alphabetically again (on vertical labels)\n",
    "    r1 = r_[[r_voc.index(w) for w in valid_test_rels]]\n",
    "\n",
    "    print(report_to_string(r, valid_test_rels), file=open('reports/o_matrix_fold_{}.csv'.format(cv+1), 'w'))\n",
    "\n",
    "    r = report(lm2, X_test2, valid_test_rels)\n",
    "\n",
    "    r_voc = [\n",
    "        word2index[rrr[0]]\n",
    "        for rrr in r\n",
    "    ]\n",
    "    r_ = np.array([\n",
    "        rrr[3:]\n",
    "        for rrr in r\n",
    "    ])\n",
    "\n",
    "    # fix the order alphabetically again (on vertical labels)\n",
    "    r2 = r_[[r_voc.index(w) for w in valid_test_rels]]\n",
    "\n",
    "    print(report_to_string(r, valid_test_rels), file=open('reports/b_matrix_fold_{}.csv'.format(cv+1), 'w'))\n",
    "\n",
    "    ######## Report on preprocessing effect on frequencies ########\n",
    "    vocab_freq_after_preprocessing = Counter([\n",
    "        vocab[w]\n",
    "        for sent in X\n",
    "        for w in sent\n",
    "        if vocab[w] in set(composit2simple.values())\n",
    "    ])\n",
    "\n",
    "    vocab_freq_after_preprocessing_train = Counter([\n",
    "        vocab[w]\n",
    "        for sent in X_train\n",
    "        for w in sent\n",
    "        if vocab[w] in set(composit2simple.values())\n",
    "    ])\n",
    "\n",
    "    vocab_freq_after_preprocessing_test = Counter([\n",
    "        vocab[w]\n",
    "        for sent in X_test\n",
    "        for w in sent\n",
    "        if vocab[w] in set(composit2simple.values())\n",
    "    ])\n",
    "\n",
    "    vocab_freq_after_preprocessing_train2 = Counter([\n",
    "        vocab[w]\n",
    "        for sent in X_train2\n",
    "        for w in sent\n",
    "        if vocab[w] in set(composit2simple.values())\n",
    "    ])\n",
    "\n",
    "    vocab_freq_after_preprocessing_test2 = Counter([\n",
    "        vocab[w]\n",
    "        for sent in X_test2\n",
    "        for w in sent\n",
    "        if vocab[w] in set(composit2simple.values())\n",
    "    ])\n",
    "    \n",
    "    freq_report_file = open('reports/frequencies_fold_{}.csv'.format(cv+1), 'w')\n",
    "    print(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\".format('term', 'dataset', 'pre-processing', 'train_original', 'train_balanced', 'test_source', 'test_sampled'),file=freq_report_file)\n",
    "    for w,freq in vocab_freq_after_preprocessing.items():\n",
    "        print(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\".format(\n",
    "            w,\n",
    "            vocab_ferq[w],\n",
    "            freq,\n",
    "            vocab_freq_after_preprocessing_train[w],\n",
    "            vocab_freq_after_preprocessing_train2[w],\n",
    "            vocab_freq_after_preprocessing_test[w],\n",
    "            vocab_freq_after_preprocessing_test2[w],\n",
    "        ),file=freq_report_file)\n",
    "    freq_report_file.close()\n",
    "    \n",
    "    np.save('saved_data/train_fold{}.npy'.format(cv+1), X_train)\n",
    "    np.save('saved_data/train2_fold{}.npy'.format(cv+1), X_train2)\n",
    "    np.save('saved_data/test_fold{}.npy'.format(cv+1), X_test)\n",
    "    np.save('saved_data/test2_fold{}.npy'.format(cv+1), X_test2)\n",
    "\n",
    "    lm1.save('saved_data/lm_o_fold{}.h5'.format(cv+1))\n",
    "    lm2.save('saved_data/lm_b_fold{}.h5'.format(cv+1))\n",
    "    \n",
    "np.save('saved_data/vocabulary.npy', vocab)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for cv in range(10):\n",
    "    lm1 = load_model('saved_data00/lm_o_fold{}.h5'.format(cv+1))\n",
    "    lm2 = load_model('saved_data00/lm_b_fold{}.h5'.format(cv+1))\n",
    "    \n",
    "    X_train2 = np.load('saved_data00/train2_fold{}.npy'.format(cv+1))\n",
    "    X_test2 = np.load('saved_data00/test2_fold{}.npy'.format(cv+1))\n",
    "\n",
    "    \n",
    "    # these functions could be optimised, in this shape they are using some global variables ...\n",
    "    def report(lm, X_test, rels):\n",
    "        X_test_spl = [\n",
    "            [w for w in sent if w in rels][0]\n",
    "            for sent in X_test\n",
    "        ]\n",
    "\n",
    "        results_sum= defaultdict(int)\n",
    "        results_count= defaultdict(float)\n",
    "        results_sum_x = defaultdict(lambda: np.zeros(len(rels)))\n",
    "\n",
    "        X_test_X = [\n",
    "            np.array([[X_test[i][0],X_test[i][1], r, X_test[i][3], X_test[i][4]] for r in rels])\n",
    "            for i,rel in enumerate(X_test_spl)\n",
    "        ]\n",
    "\n",
    "        for term, pp, xchanges in zip(X_test_spl, list(perplexity(lm, X_test).flatten()), X_test_X):\n",
    "            results_sum_x[term] += np.log2(perplexity(lm, xchanges).flatten())\n",
    "            results_sum[term] += np.log2(pp)\n",
    "            results_count[term] += 1\n",
    "\n",
    "        results = [\n",
    "            [vocab[term], 2**(results_sum[term]/results_count[term]), results_count[term]] + \\\n",
    "            [2**(results_sum_x[term][r]/results_count[term]) for r in range(len(rels))]\n",
    "            for term in results_sum\n",
    "        ]\n",
    "\n",
    "        #results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return(results)\n",
    "\n",
    "    def report_to_string(results, rels):\n",
    "        output = ''\n",
    "        output += (\"{}\\t{}\\t{}\"+\"\\t{}\"*len(rels)).format(*(['term', 'sample_pp', 'sample_size']+[vocab[rel] for rel in rels])) + '\\n'\n",
    "        for x in results:\n",
    "            output += (\"{}\\t{:.2f}\\t{}\"+\"\\t{:.2f}\"*len(rels)).format(*x) + '\\n'\n",
    "\n",
    "        return output\n",
    "    \n",
    "    ######### generate the reports #########\n",
    "    r = report(lm1, X_train2, valid_test_rels)\n",
    "\n",
    "    r_voc = [\n",
    "        word2index[rrr[0]]\n",
    "        for rrr in r\n",
    "    ]\n",
    "    r_ = np.array([\n",
    "        rrr[3:]\n",
    "        for rrr in r\n",
    "    ])\n",
    "\n",
    "    # fix the order alphabetically again (on vertical labels)\n",
    "    r1 = r_[[r_voc.index(w) for w in valid_test_rels]]\n",
    "\n",
    "    print(report_to_string(r, valid_test_rels), file=open('reports00/o_train_matrix_fold_{}.csv'.format(cv+1), 'w'))\n",
    "\n",
    "    r = report(lm2, X_train2, valid_test_rels)\n",
    "\n",
    "    r_voc = [\n",
    "        word2index[rrr[0]]\n",
    "        for rrr in r\n",
    "    ]\n",
    "    r_ = np.array([\n",
    "        rrr[3:]\n",
    "        for rrr in r\n",
    "    ])\n",
    "\n",
    "    # fix the order alphabetically again (on vertical labels)\n",
    "    r2 = r_[[r_voc.index(w) for w in valid_test_rels]]\n",
    "\n",
    "    print(report_to_string(r, valid_test_rels), file=open('reports00/b_train_matrix_fold_{}.csv'.format(cv+1), 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
